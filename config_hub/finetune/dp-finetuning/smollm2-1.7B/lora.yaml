checkpoint_dir: checkpoints/HuggingFaceTB/SmolLM2-1.7B-Instruct
data:
  class_path: litgpt.data.DeceptivePatterns
  init_args:
    ignore_index: -100
    mask_prompt: false
    num_workers: 4
    prompt_style: qwen2.5-dp
    seed: 42
    val_split_fraction: 0.03847
devices: 1
eval:
  final_validation: false
  initial_validation: false
  interval: 25000000
  max_iters: 100
  max_new_tokens: 1024
logger_name: csv
lora_alpha: 16
lora_dropout: 0.1
lora_head: true
lora_key: true
lora_mlp: true
lora_projection: true
lora_query: true
lora_r: 8
lora_value: true
num_nodes: 1
optimizer:
  class_path: torch.optim.AdamW
  init_args:
    betas:
    - 0.9
    - 0.95
    lr: 0.0002
    weight_decay: 0.0
out_dir: out/finetune/lora-smollm2-1.7b
precision: bf16-true
quantize: null
seed: 1337
train:
  epochs: 2
  global_batch_size: 6
  log_interval: 1
  lr_warmup_steps: 200
  max_norm: null
  max_seq_length: 6144
  max_steps: null
  max_tokens: null
  micro_batch_size: 1
  min_lr: 6.0e-05
  save_interval: 800
  tie_embeddings: null
